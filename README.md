# Project -1 : Linear Regression with Iris Dataset (MLflow Tracking) 

## ğŸ“Œ Project Status
ğŸš§ **Work in Progress**  
This project is under active development. Core setup and MLflow integration are completed, while model improvements and evaluation are still ongoing.

---

## ğŸ“– Project Overview
This repository demonstrates a **Linear Regression model** built using the **Iris dataset**, with experiment tracking and model inference managed through **MLflow**.

The project focuses on:
- Setting up a clean Python development environment
- Training a regression model on the Iris dataset
- Tracking experiments using MLflow
- Logging and inferencing model artifacts with MLflow
- Inferencing model Artigacts with MLFLOW inferncing, Tracking parameters
- Comparing Diff models vs metrics
- Validate tyhe model before deployment via Inferencing,
- And load model back prediction as Generative python function (MLFLOW.pyfunc)
- Register model in MLFLOW: version, tags, and Aliase
- Inferencing from model registry: Model, parameters, ,model_uri path, prediction values

---

## ğŸ› ï¸ Technologies Used
- Python
- Scikit-learn
- Pandas
- NumPy
- MLflow
- VS Code
- Virtual Environment (`venv`)

---

## Project -2 :   House Price Prediction with MLflow Tracking 

## ğŸ“Œ Project Overview 
This project demonstrates an **end-to-end machine learning workflow** for **house price prediction** using the **California Housing dataset** and **MLflow** for experiment tracking, hyperparameter tuning, and model registration.

The main goals of this project are: 
- Train a regression model with hyperparameter tuning 
- Track all experiments, parameters, and metrics using MLflow
- Compare multiple runs in the MLflow UI 
- Register the best-performing model in the MLflow Model Registry 

--- 

## ğŸ“Š Dataset 
- **California Housing Dataset** 
- Source: `sklearn.datasets.fetch_california_housing` 
- Number of samples: **20,640**
- Features:  - MedInc (Median Income),  - HouseAge, - AveRooms, - AveBedrms 
  - Population, - AveOccup, - Latitude, - Longitude 
- Target: 
  - **Price** (Median house value in units of $100,000) 

---
## ğŸ§  Model 
- Algorithm: **Random Forest Regressor** 
- Evaluation metric: **Mean Squared Error (MSE)** 
- Hyperparameter tuning: **GridSearchCV** 

---

## âœ… Project Workflow 

### 1ï¸âƒ£ Data Loading 
- Dataset loaded using `fetch_california_housing` 
- Converted into a pandas DataFrame 
- Target variable added as `Price` 

---

### 2ï¸âƒ£ Data Preparation 
- Independent variables (`X`) created by dropping the `Price` column 
- Dependent variable (`y`) set as `Price` 
- Train-test split performed (80% training / 20% testing) 

---

### 3ï¸âƒ£ Hyperparameter Tuning 
- Hyperparameter tuning implemented using `GridSearchCV` 
- Parameters tuned: 
  - `n_estimators` , - `max_depth` , - `min_samples_split`, - `min_samples_leaf`, - 3-fold cross-validation used 
- Scoring metric: `neg_mean_squared_error` 

---

### 4ï¸âƒ£ Model Training & Evaluation 
- Best model selected from GridSearchCV 
- Predictions generated on test data 
- Mean Squared Error calculated 

---

### 5ï¸âƒ£ MLflow Experiment Tracking 
- MLflow tracking server used (`http://127.0.0.1:5000`) 
- Logged to MLflow: Best hyperparameters, Mean Squared Error (MSE), Model artifacts  
- Model input/output signature inferred using `infer_signature` 

---

### 6ï¸âƒ£ Model Registration 
- Best-performing model registered in **MLflow Model Registry** 
- Registered model name:


### Best Model: Random Forest Regressor 
    Best Hyperparameters: 
    n_estimators: 200 
    max_depth: None 
    min_samples_split: 2 
    min_samples_leaf: 1 
    Mean Squared Error (MSE): ~0.25 
    Model successfully tracked and registered in MLflow 
---

# Project -3 ANN with MLflow â€“ Endâ€‘toâ€‘End MLOps Project

## ğŸ” Project Summary
Production-oriented **MLOps pipeline** demonstrating how to train, tune, track, register, and serve an **Artificial Neural Network (ANN)** using **MLflow**.  
The project covers the **full ML lifecycle** from experimentation to deployment-ready inference.

---

## ğŸ’¼ Why This Project Matters  
This project demonstrates hands-on experience with:
- âœ… **Experiment tracking at scale**
- âœ… **Hyperparameter optimization**
- âœ… **Model registry & versioning**
- âœ… **Reproducible ML workflows**
- âœ… **Deployment-ready model artifacts**

It reflects **real-world ML engineering and MLOps practices**, not just model training.

---

## ğŸ§  Technical Highlights

### Model
- ANN built with **Keras**
- Regression task (Wine Quality prediction)
- Feature normalization inside the model graph
- Metric-driven model selection (RMSE)

### Optimization
- **Hyperopt + TPE** for hyperparameter search
- Search space:
  - Learning rate (log-uniform)
  - Momentum (uniform)
- Best model selected automatically based on validation RMSE

---

## ğŸ§ª Experiment Tracking & Model Management
- **MLflow Experiments**
  - Parameters
  - Metrics
  - Model artifacts
- **Nested runs** for hyperparameter sweeps
- **MLflow Model Registry**
  - Versioned models
  - Promotion-ready artifacts

---

## ğŸš€ Inference & Serving Readiness
- Model loaded using **MLflow PyFunc**
- Serving input validated prior to deployment
- Compatible with:
  - REST API serving
  - Batch inference
  - Cloud ML platforms

---

## â˜ï¸ Deployment Readiness
- MLflow-compatible model format
- Can be packaged into:
  - Docker containers
  - Cloud-native serving endpoints
- Clear separation of training, evaluation, and inference

---

## ğŸ›  Tech Stack
- Python 3.10
- Keras / TensorFlow
- MLflow
- Hyperopt
- Scikit-learn
- Pandas / NumPy

---

## ğŸ“Š Key Outcomes
- Automated experiment comparison
- Best-performing ANN selected and registered
- Fully reproducible ML pipeline
- Production-aligned workflow (training â†’ registry â†’ inference)

---

## ğŸ¯ Skills Demonstrated
- MLOps & ML Engineering
- Experiment tracking & reproducibility
- Hyperparameter optimization
- Model versioning & governance
- Deployment-oriented ML design

---
# Project -4:  Machine Learning Pipeline with DVC & MLflow
ğŸ“Œ Project Overview

This project demonstrates how to build an end-to-end machine learning pipeline using DVC (Data Version Control) for data and model versioning and MLflow for experiment tracking.
The pipeline trains and evaluates a Random Forest Classifier on the Pima Indians Diabetes Dataset, following best practices for reproducibility and MLOps.

The goal of this project is to show how data, code, models, and experiments can be tracked together in a structured and scalable way. (dagshub.com)
ğŸš€ Key Features

    âœ… End-to-end ML pipeline
    âœ… Data and model versioning with DVC
    âœ… Experiment tracking with MLflow
    âœ… Reproducible pipeline stages
    âœ… Modular and scalable project structure
    âœ… Integration-ready with cloud storage (S3 / GCS / Azure)

ğŸ§° Tech Stack

    Python
    Scikit-learn
    DVC â€“ data & pipeline versioning
    MLflow â€“ experiment tracking
    Git / DagsHub â€“ code, data, and collaboration

ğŸ“‚ Project Structure
text

machinelearningpipeline/
â”‚
â”œâ”€â”€ data/                  # Raw and processed datasets (DVC tracked)
â”œâ”€â”€ src/                   # Source code for pipeline stages
â”‚   â”œâ”€â”€ data_preprocessing.py
â”‚   â”œâ”€â”€ train_model.py
â”‚   â””â”€â”€ evaluate_model.py
â”‚
â”œâ”€â”€ models/                # Trained models (DVC tracked)
â”œâ”€â”€ dvc.yaml               # DVC pipeline definition
â”œâ”€â”€ params.yaml            # Model and pipeline parameters
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md              # Project documentation

ğŸ”„ Pipeline Stages

    Data Preprocessing
        Cleans and prepares the dataset
        Splits data into training and test sets

    Model Training
        Trains a Random Forest Classifier
        Logs parameters and metrics to MLflow

    Model Evaluation
        Evaluates model performance
        Tracks evaluation metrics for comparison

â–¶ï¸ How to Run the Project
1ï¸âƒ£ Clone the Repository
bash

git clone https://dagshub.com/venky.sarangi/machinelearningpipeline
cd machinelearningpipeline

2ï¸âƒ£ Create Environment & Install Dependencies
bash

pip install -r requirements.txt

3ï¸âƒ£ Run the DVC Pipeline
bash

dvc repro

ğŸ“Š Experiment Tracking

All experiments, metrics, and model artifacts are logged using MLflow.
You can compare runs and analyze performance directly from the DagsHub UI.
â˜ï¸ Data & Model Versioning

    Datasets and models are tracked using DVC
    Supports remote storage (AWS S3, GCS, Azure, or DagsHub Storage)
    Ensures full reproducibility across environments

ğŸ¯ Use Cases

    Learning MLOps fundamentals
    Building reproducible ML workflows
    Experiment tracking and model comparison
    Production-ready ML pipeline templates

ğŸ¤ Contributing

Contributions are welcome!
Feel free to open issues or submit pull requests to improve the pipeline.
ğŸ“œ License

This project is intended for educational and learning purposes.
â­ If you find this project useful, consider starring it!

If you want, I can also:

    Tailor this README for GitHub vs DagsHub
    Add badges (Python, DVC, MLflow)
    Create a Beginner-friendly version
    Generate a project architecture diagram


# Project -5  ğŸ³ Basic Hello Docker Project 

- `docker --version` â†’ Verify Docker installation  
- `docker build -t welcome-app .` â†’ Build Docker image  
- `docker run -p 5000:5000 welcome-app` â†’ Run the container  
- `docker ps` â†’ Check running containers
